# other_processes_server/routers/model_management.py

import os
import pandas as pd
import joblib
import numpy as np
import csv
import io
import warnings
from datetime import datetime, timezone
from typing import Optional, List, Dict # Ensure List and Dict are imported

from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from pydantic import BaseModel, Field

# Suppress specific sklearn warnings if you uncomment the relevant part
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn.cluster._kmeans")
warnings.filterwarnings("ignore", category=FutureWarning)

router = APIRouter()

# --- Configuration (Adjust as per your actual saved model location) ---
# Assuming 'trained_models' directory is at the project root
# For a specific model ID, construct the path dynamically or set explicitly
MODELS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../trained_models')

# IMPORTANT: Update this to match the model_id generated by your GMMAnomalyDetector during training

MODEL_SAVE_PATH = 'C:/Users/sathy/Downloads/Anomaly_MCP_Servers/Anomaly_MCP_Servers/trained_models/gmm_model_auto_k'

# IMPORTANT: These feature names MUST exactly match the numerical_features
# that your GMMAnomalyDetector used during training. The order matters!
FEATURE_COLUMNS = [
    'sda_node_disk_read_bytes_total',
    'sr0_node_disk_read_bytes_total',
    'node_disk_written_bytes_total',
    '/dev/sda2_/boot_ext4_node_filesystem_free_bytes',
    '/dev/sda3_/_xfs_node_filesystem_free_bytes',
    '/dev/sda5_/var_xfs_node_filesystem_free_bytes',
    'tmpfs_/run/lock_tmpfs_node_filesystem_free_bytes',
    'tmpfs_/run/snapd/ns_tmpfs_node_filesystem_free_bytes',
    'tmpfs_/run/user/0_tmpfs_node_filesystem_free_bytes',
    'tmpfs_/run/user/1002_tmpfs_node_filesystem_free_bytes',
    'tmpfs_/run/user/118_tmpfs_node_filesystem_free_bytes',
    'tmpfs_/run/user/502_tmpfs_node_filesystem_free_bytes',
    'tmpfs_/run_tmpfs_node_filesystem_free_bytes',
    '/dev/sda2_/boot_ext4_node_filesystem_size_bytes',
    '/dev/sda3_/_xfs_node_filesystem_size_bytes',
    '/dev/sda5_/var_xfs_node_filesystem_size_bytes',
    'tmpfs_/run/lock_tmpfs_node_filesystem_size_bytes',
    'tmpfs_/run/snapd/ns_tmpfs_node_filesystem_size_bytes',
    'tmpfs_/run/user/0_tmpfs_node_filesystem_size_bytes',
    'tmpfs_/run/user/1002_tmpfs_node_filesystem_size_bytes',
    'tmpfs_/run/user/118_tmpfs_node_filesystem_size_bytes',
    'tmpfs_/run/user/502_tmpfs_node_filesystem_size_bytes',
    'tmpfs_/run_tmpfs_node_filesystem_size_bytes',
    'node_load1',
    'node_memory_MemAvailable_bytes',
    'node_memory_MemTotal_bytes',
    'node_memory_SwapTotal_bytes',
    'node_netstat_Tcp_CurrEstab',
    'node_network_receive_bytes_total',
    'node_network_transmit_bytes_total',
    'node_sockstat_sockets_used',
    'node_sockstat_TCP_tw',
    '0_0_node_cpu_core_throttles_total',
    '0_1_node_cpu_core_throttles_total',
    '1_0_node_cpu_core_throttles_total',
    '1_1_node_cpu_core_throttles_total',
    '2_0_node_cpu_core_throttles_total',
    '2_1_node_cpu_core_throttles_total',
    '3_0_node_cpu_core_throttles_total',
    '3_1_node_cpu_core_throttles_total',
    '4_0_node_cpu_core_throttles_total',
    '4_1_node_cpu_core_throttles_total',
    '5_0_node_cpu_core_throttles_total',
    '5_1_node_cpu_core_throttles_total',
    '6_0_node_cpu_core_throttles_total',
    '6_1_node_cpu_core_throttles_total',
    '7_0_node_cpu_core_throttles_total',
    '7_1_node_cpu_core_throttles_total'
]
# Output directory for anomaly reports/logs
OUTPUT_REPORTS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../anomaly_reports/gmm_model_auto_k')
MODEL_OUTPUT_REPORT_DIR = os.path.join(OUTPUT_REPORTS_DIR)
os.makedirs(MODEL_OUTPUT_REPORT_DIR, exist_ok=True) # Ensure directory exists

# ANOMALY_LOG_FILE updated to match the new CSV name from training
ANOMALY_LOG_FILE = 'C:/Users/sathy\Downloads/Anomaly_MCP_Servers/Anomaly_MCP_Servers/anomaly_reports/gmm_model_auto_k/all_anomalies_with_top5_metrics.csv'


def filter_and_transform_server_data(
    df: pd.DataFrame, 
    server_name: str, 
    expected_features: List[str] = FEATURE_COLUMNS
) -> List[Dict]:
    """
    Filters a DataFrame for a specific server's metrics and transforms
    column names to match the expected format for the anomaly detection API.

    Args:
        df (pd.DataFrame): The input DataFrame containing combined hardware monitoring metrics.
                           Expected to have columns like 'DCS1-Ares-host_metric_name'.
        server_name (str): The name of the server to filter for (e.g., 'DCS1-Ares').
        expected_features (List[str]): A list of feature names expected by the anomaly
                                       detection model (without server prefix).

    Returns:
        List[Dict]: A list of dictionaries, where each dictionary represents a
                    single data point for the specified server, with keys matching
                    `expected_features` and including the original 'timestamp'.
    """
    
    # Prefix to search for in columns
    prefix = f"{server_name}-host_"
    
    # Select columns relevant to the specific server, including timestamp
    # Ensure 'timestamp' is present and handled correctly
    selected_cols = ['timestamp'] + [col for col in df.columns if col.startswith(prefix)]

    if len(selected_cols) <= 1: # Only timestamp or no columns for this server
        print(f"No metrics found for server: {server_name}")
        return []

    df_server = df[selected_cols].copy()

    # Rename columns: remove the server prefix
    rename_mapping = {col: col.replace(prefix, '') for col in selected_cols if col.startswith(prefix)}
    df_server.rename(columns=rename_mapping, inplace=True)

    # Ensure all expected_features are present. If not, fill with NaN (or 0, depending on model expectation)
    # The anomaly detection API (predict_anomaly) already checks for missing features.
    # So, we'll ensure they are present for the transformation, filling with NaN.
    # The GMMAnomalyDetector's preprocess_data handles NaNs by ffill/bfill/fillna(0).
    for feature in expected_features:
        if feature not in df_server.columns:
            df_server[feature] = np.nan # Or 0, based on preprocessing strategy

    # Convert to list of dictionaries, ensuring correct feature order and timestamp
    # Exclude timestamp from the 'data_point' dict itself as the API expects it separately
    transformed_data_points = []
    for _, row in df_server.iterrows():
        # Ensure only the numerical features are in the 'data_point' dictionary
        # and they are in the exact order of FEATURE_COLUMNS if that's critical
        # (Pandas to_dict('records') preserves order for Python 3.7+ dicts if columns are ordered)
        
        # Filter for only the numerical metric columns based on expected_features
        metric_data = {col: row[col] for col in expected_features if col in row and pd.notnull(row[col])}
        
        # Fill any missing expected_features with 0 or a sensible default if they weren't in the row
        # This aligns with the GMMAnomalyDetector's preprocessing for missing values
        for feature in expected_features:
            if feature not in metric_data:
                metric_data[feature] = 0.0 # Default value for missing metrics

        transformed_data_points.append({
            "timestamp": row['timestamp'],
            "data_point": metric_data
        })
    
    return transformed_data_points


class AnomalyDetector:
    _instance = None # Singleton instance

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(AnomalyDetector, cls).__new__(cls)
            cls._instance._model_path = MODEL_SAVE_PATH
            cls._instance._gmm_model = None
            cls._instance._scaler = None
            cls._instance._pca_model = None
            cls._instance._anomaly_threshold = None # This will be set from the trained model's context
            cls._instance._load_model()
        return cls._instance

    def _load_model(self):
        """Loads the trained GMM model, scaler, and PCA model."""
        try:
            self._gmm_model = joblib.load(os.path.join(self._model_path, 'gmm_model.joblib'))
            self._scaler = joblib.load(os.path.join(self._model_path, 'scaler.joblib'))
            
            pca_path = os.path.join(self._model_path, 'pca_model.joblib')
            if os.path.exists(pca_path):
                self._pca_model = joblib.load(pca_path)
                print(f"PCA model loaded from: {pca_path}")
            else:
                self._pca_model = None
                print("No PCA model found to load.")

            # IMPORTANT: Load the exact anomaly threshold from the training process
            # For this example, hardcode the one determined in the previous step
            self._anomaly_threshold = -206.9065 # <--- UPDATED TO THE TRAINED THRESHOLD

            print(f"GMM model loaded from: {os.path.join(self._model_path, 'gmm_model.joblib')}")
            print(f"Scaler loaded from: {os.path.join(self._model_path, 'scaler.joblib')}")
            print(f"Anomaly threshold loaded: {self._anomaly_threshold}")
            
        except FileNotFoundError as e:
            raise RuntimeError(f"Model files not found. Ensure models are trained and exists in '{MODELS_DIR}'. Error: {e}")
        except Exception as e:
            raise RuntimeError(f"Error loading model components: {e}")

    def _predict_anomaly(self, data_point: Dict[str, float]) -> Dict:
        """
        Transforms a single data point and predicts if it's an anomaly.
        Returns prediction details and potentially top contributing metrics,
        including new statistics.
        """
        if self._gmm_model is None or self._scaler is None or self._anomaly_threshold is None:
            raise HTTPException(status_code=500, detail="Anomaly detection model not loaded or threshold not set.")

        # Ensure all required features are present in the incoming data
        if not all(col in data_point for col in FEATURE_COLUMNS):
            missing_cols = [col for col in FEATURE_COLUMNS if col not in data_point]
            raise HTTPException(status_code=400, detail=f"Missing required features: {missing_cols}")

        # Create a DataFrame for scaling, ensuring correct order
        df_single_point = pd.DataFrame([data_point], columns=FEATURE_COLUMNS)

        # Scale the data
        scaled_point = self._scaler.transform(df_single_point)

        # Predict anomaly score (negative log-likelihood)
        log_likelihood = self._gmm_model.score_samples(scaled_point)[0] # Raw log-likelihood
        anomaly_score = -log_likelihood # Negative log-likelihood
        
        # Calculate Probability Density
        probability_density = np.exp(log_likelihood)

        # Determine the most likely cluster/component for the data point
        cluster_id = self._gmm_model.predict(scaled_point)[0]
        
        # Get cluster responsibilities (probabilities for each component)
        cluster_responsibilities = self._gmm_model.predict_proba(scaled_point)[0]
        assigned_cluster_probability = cluster_responsibilities[cluster_id]

        # Calculate contribution of each feature to the anomaly
        assigned_component_mean = self._gmm_model.means_[cluster_id]
        deviations = np.abs(scaled_point[0] - assigned_component_mean)
        deviation_series = pd.Series(deviations, index=FEATURE_COLUMNS)
        
        # Get top 5 contributing metrics
        top_contributing_metrics = deviation_series.nlargest(5).to_dict()

        is_anomaly = anomaly_score > self._anomaly_threshold # Use the loaded threshold

        prediction_details = {
            "is_anomaly": bool(is_anomaly),
            "anomaly_score": round(anomaly_score, 4),
            "log_likelihood": round(log_likelihood, 4), # Added
            "probability_density": float(f"{probability_density:.4e}"), # Added, ensure float type
            "cluster_id": int(cluster_id),
            "assigned_cluster_probability": round(assigned_cluster_probability, 4), # Added
            "top_contributing_metrics": {
                metric: {"scaled_deviation": round(dev, 4), "original_value": round(data_point[metric], 2)}
                for metric, dev in top_contributing_metrics.items()
            }
        }
        return prediction_details

    def _log_anomaly_to_csv(
        self,
        timestamp: str,
        anomaly_score: float,
        log_likelihood: float, # Added
        probability_density: float, # Added
        cluster_id: int,
        assigned_cluster_probability: float, # Added
        top_metrics: Dict
    ):
        """Appends anomaly details to the CSV log file, including new statistics."""
        file_exists = os.path.isfile(ANOMALY_LOG_FILE)
        
        # Define CSV header with new columns
        csv_keys = [
            'Timestamp', 'Anomaly_Score', 'Log_Likelihood', 'Probability_Density',
            'Cluster', 'Assigned_Cluster_Probability', 'Metric',
            'Scaled_Deviation', 'Original_Value'
        ]

        # Prepare data to write
        rows_to_write = []
        for metric, details in top_metrics.items():
            rows_to_write.append({
                'Timestamp': timestamp,
                'Anomaly_Score': anomaly_score,
                'Log_Likelihood': log_likelihood,
                'Probability_Density': f"{probability_density:.4e}",  # Format as scientific notation
                'Cluster': cluster_id,
                'Assigned_Cluster_Probability': assigned_cluster_probability,
                'Metric': metric,
                'Scaled_Deviation': details['scaled_deviation'],
                'Original_Value': details['original_value']
            })

        try:
            file_exists = os.path.exists(ANOMALY_LOG_FILE)
            with open(ANOMALY_LOG_FILE, 'a', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=csv_keys)
                if not file_exists or os.stat(ANOMALY_LOG_FILE).st_size == 0:
                    writer.writeheader()
                writer.writerows(rows_to_write)
            print(f"âœ… Anomaly logged to {ANOMALY_LOG_FILE}")
        except Exception as e:
            print('Error')

# Initialize the singleton detector when the application starts



# --- Pydantic Models for API Endpoints ---

class DataPoint(BaseModel):
    """Represents a single data point for anomaly detection."""
    data_point: Dict[str, float] = Field(..., description="Dictionary of metric names and their numerical values.")
    timestamp: Optional[datetime] = Field(None, description="Timestamp of the data point. Defaults to current UTC time if not provided.")

class AnomalyPredictionResponse(BaseModel):
    """Response model for anomaly prediction."""
    is_anomaly: bool
    anomaly_score: float
    log_likelihood: float # Added
    probability_density: float # Added
    cluster_id: int
    assigned_cluster_probability: float # Added
    top_contributing_metrics: Dict[str, Dict[str, float]]
    timestamp: datetime
    message: str = "Anomaly prediction successful."